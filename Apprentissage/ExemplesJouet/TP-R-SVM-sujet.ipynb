{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=\"8pt\">Classification with Support Vector Machine</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "This `R` tutorial deals with Support Vector Machine (SVM) for classification. The aim is to illustrate the importance of the kernel choice and the tuning of the parameters by cross validation on different simulated data. \n",
    "\n",
    "It uses the `svm()` function of the `e1071` package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the `svm()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Load the `e1071` package. What kernels does it contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor simulation\n",
    "\n",
    "In the following, the factors $(X_i)_{1\\leq i\\leq n}$ are the same in all cases and are uniformly distributed on $[-1,1]\\times [0,1]$. The difference between the different cases lies in the boundary form which classifies the points. We simulate a training set with `n=200` observations, and a test set with `ntest=100` points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set:\n",
    "n <- 200\n",
    "x1 <- runif(n, min=-1, max=1)\n",
    "x2 <- runif(n, min=0, max=1)\n",
    "t <- seq(-1, 1, length=100)\n",
    "\n",
    "# Test set:\n",
    "ntest <- 100\n",
    "x1test <- runif(ntest, min=-1, max=1)\n",
    "x2test <- runif(ntest, min=0, max=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear boundary\n",
    "\n",
    "First, we assume that the points are linearly separated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundFunLin <- function(x1){\n",
    "  x1 + 0.5\n",
    "}\n",
    "\n",
    "# Training set\n",
    "gLin <- as.factor(x2 > boundFunLin(x1))\n",
    "dataTrainLin <- data.frame(x1 = x1, x2 = x2, g = gLin)\n",
    "plot(x2, x1, col=as.numeric(gLin))\n",
    "lines(boundFunLin(t), t, col=\"blue\", lty = \"dotted\")\n",
    "\n",
    "# Test set\n",
    "gLintest <- as.factor(x2test > boundFunLin(x1test))\n",
    "dataTestLin <- data.frame(x1 = x1test, x2 = x2test, g = gLintest)\n",
    "points(x2test, x1test, col=as.numeric(gLintest),pch=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM\n",
    "\n",
    "We first use a linear kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.model <- svm(g ~ ., data = dataTrainLin, type = \"C\", kernel = \"lin\", cross = 5)\n",
    "plot(svm.model, data = dataTrainLin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(svm.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** How many support vectors are there? How are they represented on the plot? \n",
    "What is their minimal number? Can you change the parameters, such that this minimal number is achieved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What are the `Single Accuracies` and the `Total Accuracy` in the `summary`? Why should we do some cross validation, even though there are no parameters to calibrate in the linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Verify that the fitted values are equal to the sign of the decision values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique((svm.model$decision.values<0) == (svm.model$fitted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is possible to get the normalized support vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(1,2))\n",
    "plot(x1~x2,data=svm.model$SV)\n",
    "plot(x1~x2,data=dataTrainLin[svm.model$index,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute the contingency table and accuracy rate of `svm.model` on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalization error\n",
    "pred.lin <- predict(svm.model,dataTestLin)\n",
    "table(pred.lin,dataTestLin$g) \n",
    "paste(\"Generalization error: \",100*(1-sum(diag(table(pred.lin,dataTestLin$g)))/ntest),\"%\",sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot(svm.model,dataTestLin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Comment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cubic boundary\n",
    "\n",
    "In this section, we assume that the points are separated by a cubic function (polynomial with degree 3). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundFunCub <- function(u){\n",
    "  2*u^2 - 10*u^3 + 0.5\n",
    "}\n",
    "\n",
    "# Training set\n",
    "gCub <- (x2 > boundFunCub(x1))\n",
    "gCub <- as.factor(gCub)\n",
    "dataTrainCub <- data.frame(x1 = x1, x2 = x2, g = gCub)\n",
    "plot(x2, x1, col=as.numeric(gCub))\n",
    "lines(boundFunCub(t), t, col=\"blue\", lty = \"dotted\")\n",
    "\n",
    "# Test set\n",
    "gCubtest <- as.factor(x2test > boundFunCub(x1test))\n",
    "dataTestCub <- data.frame(x1 = x1test, x2 = x2test, g = gCubtest)\n",
    "points(x2test, x1test, col=as.numeric(gCubtest),pch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a linear kernel, we obtain the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm.Cub.lin <- svm(g ~ ., data = dataTrainCub, type = \"C\", kernel = \"lin\", cross = 5)\n",
    "plot(svm.Cub.lin, data=dataTrainCub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(svm.Cub.lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What is the generalization error here? Comment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know here that the boundary is polynomial with degree 3, we can use a polynomial kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial kernel with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What are the parameters of such kernel, and what is their default value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.Cub.poly <- svm(g ~ ., data = dataTrainCub, type = \"C\", kernel = \"poly\", cross = 5)\n",
    "plot(svm.Cub.poly, data = dataTrainCub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(svm.Cub.poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Does this model seem reasonable? What should we do to improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to study the training error, we can print the contingency table of the fitted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table(svm.Cub.poly$fitted,dataTrainCub$g) # Il faudrait sur Ã©chantillon test !\n",
    "paste(\"Training error: \",round(100*(1-sum(diag(table(svm.Cub.poly$fitted,dataTrainCub$g)))/nrow(dataTrainCub)),1),\"%\",sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned model\n",
    "\n",
    "Since we know the degree in this example, we only tune the parameter `coef0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.Cub.poly.tune <- tune.svm(g ~ ., data = dataTrainCub, type = \"C\", kernel = \"poly\", coef0 = -5:5)\n",
    "plot(svm.Cub.poly.tune$best.model, data = dataTrainCub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(svm.Cub.poly.tune)\n",
    "summary(svm.Cub.poly.tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What value of `coef0` is obtained by cross validation? What is the generalization error in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the generalization error (estimated by cross validation) w.r.t. `coef0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm.Cub.poly.tune$best.parameters\n",
    "plot(svm.Cub.poly.tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Which parameter minimizes the error?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial kernel with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What are the parameters of such kernel, and what is their default value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.Cub.rad <- svm(g ~ ., data = dataTrainCub, type = \"C\", kernel = \"radial\", cross = 5)\n",
    "plot(svm.Cub.rad, data = dataTrainCub) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(svm.Cub.rad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** How much is the generalization error (estimated by cross validation) here? Does the model seem reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.Cub.rad.tune <- tune.svm(g ~ ., data = dataTrainCub, type = \"C\", kernel = \"radial\", gamma = seq(0.1, 2, by = 0.2))\n",
    "plot(svm.Cub.rad.tune$best.model, data = dataTrainCub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(svm.Cub.rad.tune)\n",
    "summary(svm.Cub.rad.tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can plot the estimated generalization error w.r.t. `gamma`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(svm.Cub.rad.tune)\n",
    "# plot(error~gamma,data=svm.Cub.rad.tune$performances,type=\"b\")\n",
    "# svm.Cub.rad.tune$best.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Which parameter minimizes the error? What is the generalization error in that case? What is its values for the default parameter? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation error (by cross validation)\n",
    "\n",
    "Comparison of the cross-validation errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paste(\"Linear kernel:\",100-svm.Cub.lin$tot.accuracy,\"%\",sep=\" \")\n",
    "paste(\"Default polynomial kernel:\",100-svm.Cub.poly$tot.accuracy,\"%\",sep=\" \")\n",
    "paste(\"Tuned polynomial kernel:\",100*(svm.Cub.poly.tune$best.performance),\"%\",sep=\" \")\n",
    "paste(\"Default radial kernel:\",100-svm.Cub.rad$tot.accuracy,\"%\",sep=\" \")\n",
    "paste(\"Tuned radial kernel:\",100*(svm.Cub.rad.tune$best.performance),\"%\",sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Which model seems best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x2test, x1test, col=as.numeric(gCubtest),pch=10)\n",
    "lines(boundFunCub(t), t, col=\"blue\", lty = \"dotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear kernel\n",
    "pred.Cub.lin <- predict(svm.Cub.lin,dataTestCub)\n",
    "table(pred.Cub.lin,dataTestCub$g) \n",
    "paste(\"Prediction error: \",round(100*(1-sum(diag(table(pred.Cub.lin,dataTestCub$g)))/ntest),1),\" %\",sep=\"\")\n",
    "plot(svm.Cub.lin,dataTestCub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default polynomial kernel\n",
    "pred.Cub.poly <- predict(svm.Cub.poly,dataTestCub)\n",
    "table(pred.Cub.poly,dataTestCub$g) \n",
    "paste(\"Prediction error: \",round(100*(1-sum(diag(table(pred.Cub.poly,dataTestCub$g)))/ntest),1),\" %\",sep=\"\")\n",
    "plot(svm.Cub.poly,dataTestCub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned polynomial kernel\n",
    "pred.Cub.poly.tune <- predict(svm.Cub.poly.tune$best.model,dataTestCub)\n",
    "table(pred.Cub.poly.tune,dataTestCub$g) \n",
    "paste(\"Prediction error: \",100*(1-sum(diag(table(pred.Cub.poly.tune,dataTestCub$g)))/ntest),\" %\",sep=\"\")\n",
    "plot(svm.Cub.poly.tune$best.model,dataTestCub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default radial kernel\n",
    "pred.Cub.rad <- predict(svm.Cub.rad,dataTestCub)\n",
    "table(pred.Cub.rad,dataTestCub$g) \n",
    "paste(\"Prediction error: \",100*(1-sum(diag(table(pred.Cub.rad,dataTestCub$g)))/ntest),\" %\",sep=\"\")\n",
    "plot(svm.Cub.rad,dataTestCub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned radial kernel\n",
    "pred.Cub.rad.tune <- predict(svm.Cub.rad.tune$best.model,dataTestCub)\n",
    "table(pred.Cub.rad.tune,dataTestCub$g) \n",
    "paste(\"Prediction error: \",round(100*(1-sum(diag(table(pred.Cub.rad.tune,dataTestCub$g)))/ntest),1),\" %\",sep=\"\")\n",
    "plot(svm.Cub.rad.tune$best.model,dataTestCub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction accuracies\n",
    "paste(\"Linear kernel:\",round(100*(1-sum(diag(table(pred.Cub.lin,dataTestCub$g)))/ntest),1),\"%\",sep=\" \")\n",
    "paste(\"Default polynomial kernel:\",round(100*(1-sum(diag(table(pred.Cub.poly,dataTestCub$g)))/ntest)),\"%\",sep=\" \")\n",
    "paste(\"Tuned polynomial kernel:\",round(100*(1-sum(diag(table(pred.Cub.poly.tune,dataTestCub$g)))/ntest)),\"%\",sep=\" \")\n",
    "paste(\"Default radial kernel:\",round(100*(1-sum(diag(table(pred.Cub.rad,dataTestCub$g)))/ntest)),\"%\",sep=\" \")\n",
    "paste(\"Tuned radial kernel:\",round(100*(1-sum(diag(table(pred.Cub.rad.tune,dataTestCub$g)))/ntest)),\"%\",sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Conclude. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Absolute value boundary\n",
    "\n",
    "In this section, we assume that the points are separated by an absolute value function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundFunAbs <- function(x1){\n",
    "  abs(x1)\n",
    "}\n",
    "\n",
    "# Training set\n",
    "gAbs <- (x2 > boundFunAbs(x1))\n",
    "gAbs <- as.factor(gAbs)\n",
    "dataTrainAbs <- data.frame(x1 = x1, x2 = x2, g = gAbs)\n",
    "plot(x2, x1, col=as.numeric(gAbs))\n",
    "lines(boundFunAbs(t), t, col=\"blue\", lty = \"dotted\")\n",
    "\n",
    "# Test set\n",
    "gAbstest <- as.factor(x2test > boundFunAbs(x1test))\n",
    "dataTestAbs <- data.frame(x1 = x1test, x2 = x2test, g = gAbstest)\n",
    "points(x2test, x1test, col=as.numeric(gAbstest),pch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial kernel with default parameters\n",
    "\n",
    "**Q:** Train a first SVM classifier `svm.Abs.rad` based on a radial kernel with default parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial kernel with `cost=1000`\n",
    "\n",
    "**Q:** Train a second SVM classifier `svm.Abs.rad.cost1000` based on a radial kernel with default parameters, with `cost=1000`. What can you observe? \n",
    "What is the effect of the cost w.r.t. overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned radial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Tune a third model `svm.Abs.rad.tune` based on a radial kernel by cross validation. The parameters can be taken in the following grids: \n",
    "- `gamma = seq(0.1, 2, by = 0.2)`, \n",
    "- `cost = c(1, 25, 50, 75, 100, 150, 200)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** For which parameters is the generalization error the smallest? What is its value in that case? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Plot the generalization error w.r.t `gamma` and `cost`. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set prediction\n",
    "\n",
    "**Q:** Compare the three radial models `svm.Abs.rad`, `svm.Abs.rad.tune.cost1000` and `svm.Abs.rad.tune` on the test set (in terms of contingency table and generalization error). Comment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sine boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boundFunSin <- function(x1){\n",
    "  sin(2*pi*x1)\n",
    "}\n",
    "\n",
    "# Training set\n",
    "gSin <- (x2 > boundFunSin(x1))\n",
    "gSin <- as.factor(gSin)\n",
    "dataTrainSin <- data.frame(x1 = x1, x2 = x2, g = gSin)\n",
    "plot(x2, x1, col=as.numeric(gSin))\n",
    "lines(boundFunSin(t), t, col=\"blue\", lty = \"dotted\")\n",
    "\n",
    "# Test set\n",
    "gSintest <- as.factor(x2test > boundFunSin(x1test))\n",
    "dataTestSin <- data.frame(x1 = x1test, x2 = x2test, g = gSintest)\n",
    "points(x2test, x1test, col=as.numeric(gSintest),pch=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Same questions as for the Absolute value boundary case (with radial kernels). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With `cost=1000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation error (by cross validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disk boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundFunDisk <- function(x1, x2){\n",
    "   x1^2 + (x2 - 1/2)^2\n",
    "}\n",
    "r <- 0.4\n",
    "\n",
    "# Training set\n",
    "gDisk <- as.factor(boundFunDisk(x1, x2) < r^2)\n",
    "dataTrainDisk <- data.frame(x1 = x1, x2 = x2, g = gDisk)\n",
    "plot(x2, x1, col=as.numeric(gDisk))\n",
    "lines(1/2 + r * cos(2*pi*t), r * sin(2*pi*t), col=\"blue\", lty = \"dotted\")\n",
    "\n",
    "# Test set\n",
    "gDisktest <- as.factor(boundFunDisk(x1test, x2test) < r^2)\n",
    "dataTestDisk <- data.frame(x1 = x1test, x2 = x2test, g = gDisktest)\n",
    "points(x2test, x1test, col=as.numeric(gDisktest),pch=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Same questions as for the Absolute value boundary case (with polynomial and radial kernels). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation error (by cross validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
